{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>  Решение тестового задания при помощи модели SBERT и NLTK</center>","metadata":{"id":"gtd4BmBkPr6e"}},{"cell_type":"markdown","source":"Задача: найти фрагмент в тексте по заданному запросу. Для решения используется алгоритм, используюий подход поисковой системы. Вместо документов - отдельные предложения. Создаются эмбединги запроса и предложений, а модель возвращает предложение, наиболее похожее на запрос.\n\n**Предобработка текста**\n\nПусть искомый фрагмент - предложение из текста. В таком случае требуется разделить текст на предложения. Проблема заключается в том, что некоторые предложения не разделены знаком припинания. Для решения этой проблемы используются регулярные выражения и библиотека NLTK. В будущем для улучшения модели можно создать отдельную модель, которая бы расставляла недостающие знаки препинания\n\n**Получения эмбедингов**\n\nДля создания эмбедингов предложений используется модель [SBERT](https://arxiv.org/pdf/1908.10084.pdf). Модель принимает на вход предложения и возвращает эмбединги предложений.\n\n**Тюнинг модели**\n\nНа основе прогноза модель дообучается. При помощи косинусной меры определяется похожесть предложений. Наиболее похожее предложение сохраняется в качестве прогноза. Если прогноз совпал с ответом, то моделе сообщается, что найденный вектор и запрос схожи, если нет - то сообщается что у них низкая схожесть. Для определения схожести используется косинусная мера, для определения схожести ответа и прогноза - квадрат растояния Джаро (т.к. требуется понять, сколько символов совпало и не сильно штрафовать за промах в несколько символов)","metadata":{}},{"cell_type":"markdown","source":"## Установка модулей","metadata":{"id":"Xy_evZRtP5zQ"}},{"cell_type":"code","source":"!pip install -qU transformers sentence-transformers","metadata":{"id":"V-ERwCBStGEM","execution":{"iopub.status.busy":"2023-05-04T11:37:57.860658Z","iopub.execute_input":"2023-05-04T11:37:57.861118Z","iopub.status.idle":"2023-05-04T11:38:13.773779Z","shell.execute_reply.started":"2023-05-04T11:37:57.861080Z","shell.execute_reply":"2023-05-04T11:38:13.772099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{"id":"GtC8EWI1Gfho","outputId":"f26dd0c2-774d-44db-939e-8f5d5a26505d","execution":{"iopub.status.busy":"2023-05-04T11:38:13.776513Z","iopub.execute_input":"2023-05-04T11:38:13.777006Z","iopub.status.idle":"2023-05-04T11:38:24.821961Z","shell.execute_reply.started":"2023-05-04T11:38:13.776952Z","shell.execute_reply":"2023-05-04T11:38:24.820412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install jellyfish","metadata":{"execution":{"iopub.status.busy":"2023-05-04T11:38:24.823929Z","iopub.execute_input":"2023-05-04T11:38:24.824286Z","iopub.status.idle":"2023-05-04T11:38:36.222093Z","shell.execute_reply.started":"2023-05-04T11:38:24.824252Z","shell.execute_reply":"2023-05-04T11:38:36.220931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Импорт библиотек и данных","metadata":{"id":"v3c0smxfP8QV"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"id":"GNAa1Ie8kK40","execution":{"iopub.status.busy":"2023-05-04T11:38:36.225286Z","iopub.execute_input":"2023-05-04T11:38:36.225681Z","iopub.status.idle":"2023-05-04T11:38:36.233792Z","shell.execute_reply.started":"2023-05-04T11:38:36.225642Z","shell.execute_reply":"2023-05-04T11:38:36.232409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.stem import WordNetLemmatizer \n\nnltk.download('punkt')\nnltk.download('wordnet')","metadata":{"id":"tLG5Fx2eF82g","outputId":"ba4181d1-3e6b-438f-cd3a-a17f036467ec","execution":{"iopub.status.busy":"2023-05-04T11:38:36.235108Z","iopub.execute_input":"2023-05-04T11:38:36.235527Z","iopub.status.idle":"2023-05-04T11:38:37.693955Z","shell.execute_reply.started":"2023-05-04T11:38:36.235497Z","shell.execute_reply":"2023-05-04T11:38:37.692858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nimport tqdm\nimport re","metadata":{"id":"vjeXzXzfkPaK","execution":{"iopub.status.busy":"2023-05-04T11:38:37.694902Z","iopub.execute_input":"2023-05-04T11:38:37.695210Z","iopub.status.idle":"2023-05-04T11:38:37.700504Z","shell.execute_reply.started":"2023-05-04T11:38:37.695184Z","shell.execute_reply":"2023-05-04T11:38:37.699480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom jellyfish import jaro_distance","metadata":{"id":"iDTOvDU7tGEd","execution":{"iopub.status.busy":"2023-05-04T11:38:37.701925Z","iopub.execute_input":"2023-05-04T11:38:37.702243Z","iopub.status.idle":"2023-05-04T11:38:50.698718Z","shell.execute_reply.started":"2023-05-04T11:38:37.702215Z","shell.execute_reply":"2023-05-04T11:38:50.697641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/nlp-task-2023-data/train.json', 'r', encoding='utf-8') as f:\n    train_raw = json.loads(f.read())\n\ntrain = pd.json_normalize(train_raw)\ntrain.columns = train.columns.str.replace('.', '_')\ntrain['text'] = train['text'].astype(str)\ntrain['label'] = train['label'].astype(str)\ntrain['extracted_part_text'] = train['extracted_part_text'].str[0]\ntrain['extracted_part_answer_start'] = train['extracted_part_answer_start'].str[0].astype('int')\ntrain['extracted_part_answer_end'] = train['extracted_part_answer_end'].str[0].astype('int')\n\ntrain.head()","metadata":{"id":"_IaZtMh3kA-R","outputId":"1fdf59cd-f2b2-49ee-93fb-625c454d1e31","execution":{"iopub.status.busy":"2023-05-04T11:38:50.700619Z","iopub.execute_input":"2023-05-04T11:38:50.701668Z","iopub.status.idle":"2023-05-04T11:38:50.962184Z","shell.execute_reply.started":"2023-05-04T11:38:50.701621Z","shell.execute_reply":"2023-05-04T11:38:50.961046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"id":"URpnYvuUU-tF","outputId":"4f94a2dd-0509-4cc7-cb35-cfed899cf229","execution":{"iopub.status.busy":"2023-05-04T11:38:50.963742Z","iopub.execute_input":"2023-05-04T11:38:50.964198Z","iopub.status.idle":"2023-05-04T11:38:50.992363Z","shell.execute_reply.started":"2023-05-04T11:38:50.964156Z","shell.execute_reply":"2023-05-04T11:38:50.991168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_w_ans = train[(train['extracted_part_answer_start'] != 0) & (train['extracted_part_answer_end'] != 0)]\ntrain_w_o_ans = train[(train['extracted_part_answer_start'] == 0) & (train['extracted_part_answer_end'] == 0)]","metadata":{"execution":{"iopub.status.busy":"2023-05-04T11:38:50.997206Z","iopub.execute_input":"2023-05-04T11:38:50.997805Z","iopub.status.idle":"2023-05-04T11:38:51.007221Z","shell.execute_reply.started":"2023-05-04T11:38:50.997761Z","shell.execute_reply":"2023-05-04T11:38:51.006397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.pie(train_w_ans['label'].value_counts())\nplt.legend(train_w_ans['label'].value_counts().index)\nplt.title('Соотношение классов для запросов с ответом')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-04T11:38:51.008729Z","iopub.execute_input":"2023-05-04T11:38:51.009477Z","iopub.status.idle":"2023-05-04T11:38:51.231830Z","shell.execute_reply.started":"2023-05-04T11:38:51.009436Z","shell.execute_reply":"2023-05-04T11:38:51.230706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.pie(train_w_o_ans['label'].value_counts())\nplt.legend(train_w_o_ans['label'].value_counts().index)\nplt.title('Соотношение классов для запросов без ответа')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-04T11:38:51.233608Z","iopub.execute_input":"2023-05-04T11:38:51.234330Z","iopub.status.idle":"2023-05-04T11:38:51.457747Z","shell.execute_reply.started":"2023-05-04T11:38:51.234270Z","shell.execute_reply":"2023-05-04T11:38:51.456587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/nlp-task-2023-data/test.json', 'r', encoding='utf-8') as f:\n    test_raw = json.loads(f.read())\n\ntest = pd.json_normalize(test_raw)\ntest.columns = test.columns.str.replace('.', '_')\ntest['text'] = test['text'].astype(str)\ntest['label'] = test['label'].astype(str)\ntest.head()","metadata":{"id":"TQuYXK7DUEfy","outputId":"6bf2cff2-7dd8-4797-fa75-0cff6fcae1fd","execution":{"iopub.status.busy":"2023-05-04T11:38:51.459557Z","iopub.execute_input":"2023-05-04T11:38:51.460279Z","iopub.status.idle":"2023-05-04T11:38:51.501697Z","shell.execute_reply.started":"2023-05-04T11:38:51.460234Z","shell.execute_reply":"2023-05-04T11:38:51.500896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"id":"I3XgOeSIU8pX","outputId":"c4307648-0ca4-4f27-e8a0-49d4e0d6155e","execution":{"iopub.status.busy":"2023-05-04T11:38:51.503025Z","iopub.execute_input":"2023-05-04T11:38:51.503589Z","iopub.status.idle":"2023-05-04T11:38:51.516893Z","shell.execute_reply.started":"2023-05-04T11:38:51.503557Z","shell.execute_reply":"2023-05-04T11:38:51.515673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.pie(test['label'].value_counts())\nplt.legend(test['label'].value_counts().index)\nplt.title('Соотношение классов для запросов в тестовой выборке')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-04T11:38:51.519431Z","iopub.execute_input":"2023-05-04T11:38:51.520259Z","iopub.status.idle":"2023-05-04T11:38:51.727184Z","shell.execute_reply.started":"2023-05-04T11:38:51.520214Z","shell.execute_reply":"2023-05-04T11:38:51.725917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка модели","metadata":{"id":"xGQaD7DRQCA_"}},{"cell_type":"code","source":"if os.path.exists('/kaggle/working/model'):\n    model = SentenceTransformer('/kaggle/working/model')\nelse:\n    model = SentenceTransformer('distilbert-base-nli-mean-tokens')","metadata":{"id":"3CTxbmsk3U4t","outputId":"56377fcd-f949-47fd-da7e-2f63d989d51c","execution":{"iopub.status.busy":"2023-05-04T15:27:32.194790Z","iopub.execute_input":"2023-05-04T15:27:32.195243Z","iopub.status.idle":"2023-05-04T15:27:33.316778Z","shell.execute_reply.started":"2023-05-04T15:27:32.195195Z","shell.execute_reply":"2023-05-04T15:27:33.313981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создать новую модель\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')","metadata":{"execution":{"iopub.status.busy":"2023-05-04T11:38:58.520981Z","iopub.execute_input":"2023-05-04T11:38:58.521354Z","iopub.status.idle":"2023-05-04T11:38:59.471958Z","shell.execute_reply.started":"2023-05-04T11:38:58.521298Z","shell.execute_reply":"2023-05-04T11:38:59.470902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Вспомогательные функции","metadata":{"id":"Of6eBLaHQO1P"}},{"cell_type":"markdown","source":"Предобработка текста","metadata":{"id":"9YiQFdskQhUg"}},{"cell_type":"code","source":"def tokenize(text: str):\n    \"\"\"\n    Функция для токенизации текста. Сначала функция доставляет недостающие точки\n    при помощи шаблона, а затем токенизирует его.\n    \"\"\"\n    pattern = \"(?<=[а-яё])\\s+(?=[А-ЯЁ][^А-ЯЁ])\"\n    # Шаблон - слово с маленькой буквы, пробел, слово с большой буквы\n    text_sub = re.sub(pattern, '. ', text) \n    sents = sent_tokenize(text_sub, language='russian')\n    # Избавляемся от лишних точек \n    return [re.sub(pattern, '', sent) if sent[-1] != '.' else re.sub(pattern, '', sent)[:-1] for sent in sents]","metadata":{"id":"K0pveqWZz-8j","execution":{"iopub.status.busy":"2023-05-04T11:38:59.473418Z","iopub.execute_input":"2023-05-04T11:38:59.473752Z","iopub.status.idle":"2023-05-04T11:38:59.481563Z","shell.execute_reply.started":"2023-05-04T11:38:59.473722Z","shell.execute_reply":"2023-05-04T11:38:59.480215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, text: str, label: str, fit=False) -> [str, int, int]:\n    \"\"\"\n    Cоздание прогноза по входным данным. Возвращает фразу, начало и конец фразы в исходном тексте\n    model: модель SBERT\n    text: текст для поиска\n    label: фраза для поиска\n    \"\"\"\n    # Проверка на вхождение label в текст\n    if not fit and label not in text:\n        return '', 0, 0\n\n    # Токенизация\n    tokens = tokenize(text)\n\n\n    # Получаем эмбединг лейбла и эмбединги токенов\n    embedding_label = model.encode(label)\n    embeddings_tokens = model.encode(tokens)\n\n    # Считаем \"похожесть\" эмбедингов при помощи косинусной метрики\n    similarity = cosine_similarity(embedding_label.reshape(1, -1), \n                                 embeddings_tokens)[0]\n    # Сохраняем предсказанный токен и ищем его начало\n    predicted_token = tokens[np.argmax(similarity)]\n    if not fit:\n        predicted_start = text.find(predicted_token)\n        # Возвращаем предсказанный токен и его позицию в тексте\n        return predicted_token, predicted_start, predicted_start + len(predicted_token)\n    return predicted_token","metadata":{"execution":{"iopub.status.busy":"2023-05-04T11:38:59.483298Z","iopub.execute_input":"2023-05-04T11:38:59.483842Z","iopub.status.idle":"2023-05-04T11:38:59.492786Z","shell.execute_reply.started":"2023-05-04T11:38:59.483806Z","shell.execute_reply":"2023-05-04T11:38:59.491691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit(dataset: pd.DataFrame, model, n_epochs=5, batch_size=64, dataloader_batch_size=16):\n    \"\"\"\n    Функция тюнинга модели\n    Обучение проходит в несколько эпох, собираются прогнозы модели, а затем она доубачется на них\n    dataset: Данные без пропусков\n    n_epochs: Количество эпох для обучения\n    batch_size: Размер батча для добучения модели\n    dataloader_batch_size: Размер батча для сохранения предсказаний\n    \"\"\"\n    train_examples = []\n    for epoch in range(n_epochs):\n        for i in tqdm.tqdm(range(dataset.shape[0]), postfix=f'epoch №{epoch}'):\n            train_text = dataset.text[i]\n            train_label = dataset.label[i]\n            # Предсказывает токен\n            pred = predict(model, train_text, train_label, fit=True)\n            \n            # Используется квадрат расстояния Джаро между предсказанным и ожидаемым текстом\n            # sim = 1 - полное совпадение, sim = 0 - фразы полностью не совпадают\n            # sim устанавливается в качестве меры схожести между запросом и полученным ответом\n            sim = jaro_distance(pred, dataset.extracted_part_text[i])\n            train_examples.append(InputExample(texts=[train.label[i], pred], label=sim**2))\n        if i % batch_size == 0 and i > 0:\n            # тюнинг модели после накопления данных\n            train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=dataloader_batch_size)\n            train_loss = losses.CosineSimilarityLoss(model)\n            model.fit(train_objectives=[(train_dataloader, train_loss)], \n                      epochs=1, \n                      warmup_steps=100,\n                      output_path='/kaggle/working/model')\n            train_examples = []","metadata":{"id":"vErxdOhAnigs","execution":{"iopub.status.busy":"2023-05-04T11:38:59.494746Z","iopub.execute_input":"2023-05-04T11:38:59.495467Z","iopub.status.idle":"2023-05-04T11:38:59.507656Z","shell.execute_reply.started":"2023-05-04T11:38:59.495420Z","shell.execute_reply":"2023-05-04T11:38:59.506633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Обучение модели","metadata":{}},{"cell_type":"code","source":"# Обучение модели по датасету без пропусков\ndataset = train_w_ans.reset_index(drop=True)\nfit(dataset, model, n_epochs=3)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T15:30:01.653044Z","iopub.execute_input":"2023-05-04T15:30:01.653458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('/kaggle/working/model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Создание прогноза для тестовой выборки","metadata":{}},{"cell_type":"code","source":"expected_text = []\nexpected_start = []\nexpected_end = []\n\nfor i in tqdm.tqdm(range(test.shape[0])):\n    text = test.text[i]\n    label = test.label[i]\n    pred_token, pred_start, pred_end = predict(model, text, label)\n\n    expected_text.append(pred_token)\n    expected_start.append(pred_start)\n    expected_end.append(pred_end) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['extracted_part_text'] = expected_text\ntest['extracted_part_answer_start'] = expected_start\ntest['extracted_part_answer_end'] = expected_end","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.to_json('test_ans.json', orient=\"records\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
